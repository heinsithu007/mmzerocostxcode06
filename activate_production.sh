#!/bin/bash
# ğŸš€ One-Click Production Activation Script
# Activate actual DeepSeek R1 model with full vLLM server

set -e

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘           ğŸš€ ACTIVATING PRODUCTION DeepSeek R1 MODEL            â•‘${NC}"
echo -e "${BLUE}â•‘                One-Click Model Deployment                        â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"

# Check if vLLM is installed
echo -e "${BLUE}[1/5]${NC} Checking vLLM installation..."
if ! python3 -c "import vllm" 2>/dev/null; then
    echo -e "${YELLOW}Installing vLLM...${NC}"
    pip install vllm
fi
echo -e "${GREEN}âœ… vLLM ready${NC}"

# Detect system capabilities
echo -e "${BLUE}[2/5]${NC} Detecting system capabilities..."
SYSTEM_INFO=$(python3 -c "
import torch
import psutil
import json

has_gpu = torch.cuda.is_available()
total_ram = psutil.virtual_memory().total // (1024**3)
cpu_cores = psutil.cpu_count()

if has_gpu:
    gpu_memory = torch.cuda.get_device_properties(0).total_memory // (1024**3)
    config = 'gpu' if gpu_memory >= 8 else 'cpu'
else:
    config = 'cpu'

print(json.dumps({
    'config': config,
    'has_gpu': has_gpu,
    'total_ram': total_ram,
    'cpu_cores': cpu_cores
}))
")

CONFIG=$(echo $SYSTEM_INFO | python3 -c "import sys, json; print(json.load(sys.stdin)['config'])")
echo -e "${GREEN}âœ… Configuration: $CONFIG${NC}"

# Start vLLM server
echo -e "${BLUE}[3/5]${NC} Starting vLLM server with DeepSeek R1..."

if [ "$CONFIG" = "gpu" ]; then
    echo -e "${GREEN}ğŸ¯ GPU Mode: High-performance deployment${NC}"
    nohup vllm serve deepseek-ai/DeepSeek-R1-0528 \
        --host 0.0.0.0 \
        --port 8000 \
        --tensor-parallel-size 1 \
        --gpu-memory-utilization 0.8 \
        --max-model-len 32768 \
        --trust-remote-code > vllm_server.log 2>&1 &
else
    echo -e "${YELLOW}ğŸ¯ CPU Mode: Optimized deployment${NC}"
    nohup vllm serve deepseek-ai/DeepSeek-R1-0528 \
        --host 0.0.0.0 \
        --port 8000 \
        --device cpu \
        --max-model-len 16384 \
        --trust-remote-code > vllm_server.log 2>&1 &
fi

VLLM_PID=$!
echo "vLLM PID: $VLLM_PID" > vllm.pid

# Wait for server to be ready
echo -e "${BLUE}[4/5]${NC} Waiting for model to load..."
echo -e "${YELLOW}â³ This may take 2-5 minutes depending on your system...${NC}"

for i in {1..60}; do
    if curl -s http://localhost:8000/health > /dev/null 2>&1; then
        echo -e "${GREEN}âœ… vLLM server is ready!${NC}"
        break
    fi
    echo -n "."
    sleep 5
done

# Update backend to use production mode
echo -e "${BLUE}[5/5]${NC} Activating production mode..."
curl -s -X POST http://localhost:12000/api/v2/vllm/start > /dev/null

# Verify deployment
echo -e "${BLUE}ğŸ” Verifying deployment...${NC}"
RESPONSE=$(curl -s -X POST http://localhost:12000/api/v2/generate-code \
    -H "Content-Type: application/json" \
    -d '{"prompt":"Hello world in Python","language":"python","complexity":"simple"}')

if echo "$RESPONSE" | grep -q "success"; then
    echo -e "${GREEN}âœ… Production deployment successful!${NC}"
else
    echo -e "${RED}âŒ Deployment verification failed${NC}"
    exit 1
fi

# Display results
echo
echo -e "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${GREEN}â•‘                    ğŸ‰ PRODUCTION ACTIVATED! ğŸ‰                  â•‘${NC}"
echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo
echo -e "${BLUE}ğŸš€ DeepSeek R1 Model Status:${NC} ${GREEN}ACTIVE${NC}"
echo -e "${BLUE}ğŸ”— Platform URL:${NC} http://localhost:12000"
echo -e "${BLUE}ğŸ“Š vLLM Server:${NC} http://localhost:8000"
echo -e "${BLUE}ğŸ”’ Privacy:${NC} ${GREEN}100% Local - No external API calls${NC}"
echo -e "${BLUE}ğŸ’° Cost:${NC} ${GREEN}Local deployment - No ongoing fees${NC}"
echo
echo -e "${YELLOW}ğŸ“‹ Enterprise Features Now Active:${NC}"
echo "   â€¢ âœ… Full DeepSeek R1 reasoning capabilities"
echo "   â€¢ âœ… Advanced code generation with thinking process"
echo "   â€¢ âœ… Comprehensive code analysis and review"
echo "   â€¢ âœ… Intelligent chat with programming expertise"
echo "   â€¢ âœ… Project-level analysis and recommendations"
echo "   â€¢ âœ… Multi-agent collaboration workflows"
echo "   â€¢ âœ… Real-time performance optimization"
echo
echo -e "${BLUE}ğŸ¯ Quick Test:${NC}"
echo "Visit http://localhost:12000 and try:"
echo "1. Generate complex code with advanced reasoning"
echo "2. Analyze code with detailed explanations"
echo "3. Chat about advanced programming concepts"
echo
echo -e "${YELLOW}âš ï¸  Management Commands:${NC}"
echo "â€¢ Stop vLLM: kill \$(cat vllm.pid)"
echo "â€¢ View logs: tail -f vllm_server.log"
echo "â€¢ Restart: ./activate_production.sh"
echo
echo -e "${GREEN}ğŸŠ Enjoy your fully activated Enhanced CodeAgent platform!${NC}"