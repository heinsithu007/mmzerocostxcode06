# Enhanced CodeAgent Integration v2.0 - Production Docker Compose
# Supports CPU/GPU adaptive deployment with cost-free demo mode

version: '3.8'

services:
  # Enhanced Backend v2.0
  enhanced-backend-v2:
    build:
      context: ./backend-v2
      dockerfile: Dockerfile
    container_name: enhanced-codeagent-backend-v2
    ports:
      - "12000:12000"
    volumes:
      - ./workspace:/app/workspace
      - ./logs:/app/logs
      - ./config:/app/config
      - model_cache:/app/models
    environment:
      - PYTHONPATH=/app
      - LOG_LEVEL=INFO
      - DEMO_MODE=true
      - VLLM_HOST=0.0.0.0
      - VLLM_PORT=8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:12000/api/v2/status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - codeagent-network

  # Optional: vLLM Server (CPU-optimized)
  vllm-server-cpu:
    image: vllm/vllm-openai:latest
    container_name: enhanced-codeagent-vllm-cpu
    command: [
      "--model", "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--device", "cpu",
      "--max-model-len", "4096",
      "--trust-remote-code"
    ]
    ports:
      - "8000:8000"
    volumes:
      - model_cache:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - OMP_NUM_THREADS=2
      - HF_HOME=/root/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
    profiles: ["vllm-cpu"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    networks:
      - codeagent-network

  # Optional: vLLM Server (GPU-optimized)
  vllm-server-gpu:
    image: vllm/vllm-openai:latest
    container_name: enhanced-codeagent-vllm-gpu
    command: [
      "--model", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--tensor-parallel-size", "1",
      "--gpu-memory-utilization", "0.8",
      "--max-model-len", "8192",
      "--trust-remote-code"
    ]
    ports:
      - "8000:8000"
    volumes:
      - model_cache:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_HOME=/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: ["vllm-gpu"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - codeagent-network

  # Enhanced Frontend v2.0 (Optional)
  enhanced-frontend-v2:
    build:
      context: ./frontend-v2
      dockerfile: Dockerfile
    container_name: enhanced-codeagent-frontend-v2
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_API_URL=http://localhost:12000
    depends_on:
      - enhanced-backend-v2
    profiles: ["frontend"]
    restart: unless-stopped
    networks:
      - codeagent-network

  # Redis Cache (Optional for production scaling)
  redis:
    image: redis:7-alpine
    container_name: enhanced-codeagent-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    profiles: ["cache"]
    restart: unless-stopped
    networks:
      - codeagent-network

  # Nginx Load Balancer (Optional for production)
  nginx:
    image: nginx:alpine
    container_name: enhanced-codeagent-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/ssl
    depends_on:
      - enhanced-backend-v2
    profiles: ["production"]
    restart: unless-stopped
    networks:
      - codeagent-network

  # Monitoring (Optional)
  prometheus:
    image: prom/prometheus:latest
    container_name: enhanced-codeagent-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    profiles: ["monitoring"]
    restart: unless-stopped
    networks:
      - codeagent-network

  grafana:
    image: grafana/grafana:latest
    container_name: enhanced-codeagent-grafana
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    profiles: ["monitoring"]
    restart: unless-stopped
    networks:
      - codeagent-network

volumes:
  model_cache:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  codeagent-network:
    driver: bridge
    name: enhanced-codeagent-network

# Usage Examples:
#
# Basic deployment (demo mode, cost-free):
# docker-compose -f docker-compose-v2.yml up enhanced-backend-v2
#
# With CPU vLLM server:
# docker-compose -f docker-compose-v2.yml --profile vllm-cpu up
#
# With GPU vLLM server:
# docker-compose -f docker-compose-v2.yml --profile vllm-gpu up
#
# Full production stack:
# docker-compose -f docker-compose-v2.yml --profile production --profile cache --profile monitoring up
#
# Development with frontend:
# docker-compose -f docker-compose-v2.yml --profile frontend up